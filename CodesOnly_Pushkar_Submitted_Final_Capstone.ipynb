{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x_0e8YO36egD"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel('Telco_customer_churn_demographics.xlsx')\n",
        "df2 = pd.read_excel('Telco_customer_churn_location.xlsx')\n",
        "df3 = pd.read_excel('Telco_customer_churn_services.xlsx')\n",
        "df4 = pd.read_excel('Telco_customer_churn_status.xlsx')\n",
        "df5 = pd.read_excel('Telco_customer_churn_population.xlsx')\n",
        "\n",
        "df1.columns = df1.columns.str.replace(' ', '_')\n",
        "df2.columns = df2.columns.str.replace(' ', '_')\n",
        "df3.columns = df3.columns.str.replace(' ', '_')\n",
        "df4.columns = df4.columns.str.replace(' ', '_')\n",
        "df5.columns = df5.columns.str.replace(' ', '_')"
      ],
      "metadata": {
        "id": "_FaWzU_Z8DXf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', 200)"
      ],
      "metadata": {
        "id": "deqQixFT8DUs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "O5QakNMd8DSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "c_SApA9C8DPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.head()"
      ],
      "metadata": {
        "id": "_AMamhc98DMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.head()"
      ],
      "metadata": {
        "id": "bYmlSScI8C0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.head()"
      ],
      "metadata": {
        "id": "Llz9fTnX8CxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('df1 has the shape of: ', df1.shape)\n",
        "print('df2 has the shape of: ', df2.shape)\n",
        "print('df3 has the shape of: ', df3.shape)\n",
        "print('df4 has the shape of: ', df4.shape)\n",
        "print('df5 has the shape of: ', df5.shape)"
      ],
      "metadata": {
        "id": "DqL_A8wj8CuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df1, df2, on='Customer_ID', how='outer')\n",
        "print(merged_df.shape)"
      ],
      "metadata": {
        "id": "hVp1KB2w8Crj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df1 = pd.merge(merged_df, df3, on='Customer_ID', how='outer')\n",
        "print(merged_df1.shape)"
      ],
      "metadata": {
        "id": "XFzb4gfW8Cos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df2 = pd.merge(merged_df1, df4, on='Customer_ID', how='outer')\n",
        "print(merged_df2.shape)"
      ],
      "metadata": {
        "id": "CUtFJ3Cx8Cl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df3 = pd.merge(merged_df2, df5, on='Zip_Code', how='outer')\n",
        "print(merged_df3.shape)"
      ],
      "metadata": {
        "id": "bbP-WHEL8Ci5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df3.head()"
      ],
      "metadata": {
        "id": "kCmcppoY8Cfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_check_df = merged_df3.isnull().any()\n",
        "nan_check_df.shape"
      ],
      "metadata": {
        "id": "0eh9kckr8Ccs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_nan_count = merged_df3.isnull().sum()\n",
        "total_nan_count"
      ],
      "metadata": {
        "id": "9CjuEMJe8CZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_zip_df2 = df2['Zip_Code'].nunique()\n",
        "print(\"Unique values of Zip_Code in df2:\", unique_zip_df2)"
      ],
      "metadata": {
        "id": "EKIPGLGMOKzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_zip_df5 = df5['Zip_Code'].nunique()\n",
        "print(\"Unique values of Zip_Code in df5:\", unique_zip_df5)"
      ],
      "metadata": {
        "id": "iglcmC4aOKuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_unique_values1 = merged_df2['Zip_Code'].nunique()\n",
        "print(\"Number of unique values:\", num_unique_values1)"
      ],
      "metadata": {
        "id": "uHCzPMeL8CWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_unique_values2 = merged_df3['Zip_Code'].nunique()\n",
        "print(\"Number of unique values:\", num_unique_values2)"
      ],
      "metadata": {
        "id": "wLC-H4XX8CS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Difference of unique values: ', num_unique_values2 - num_unique_values1)"
      ],
      "metadata": {
        "id": "RQ3CQqXV9fSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df3.dropna(subset=['Customer_ID'], inplace=True)"
      ],
      "metadata": {
        "id": "IOKHro_n9fPg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_count = merged_df3.isna().sum()\n",
        "print(nan_count)"
      ],
      "metadata": {
        "id": "PU9uixKY9fMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df3.columns"
      ],
      "metadata": {
        "id": "an-RB6Ab9fJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_to_remove = ['Customer_ID', 'Count_x', 'Under_30', 'Number_of_Dependents','Location_ID',\n",
        "                 'Count_y', 'Country', 'State', 'City', 'Zip_Code', 'Lat_Long', 'Latitude',\n",
        "                 'Longitude', 'Service_ID', 'Count_x', 'Quarter_x', 'Number_of_Referrals',\n",
        "                 'Total_Refunds', 'Status_ID', 'Count_y', 'Quarter_y', 'Satisfaction_Score',\n",
        "                 'Customer_Status', 'Churn_Value', 'Churn_Score', 'CLTV', 'Churn_Category',\n",
        "                 'Churn_Reason', 'ID', 'Population']\n",
        "\n",
        "mer_fin = merged_df3.drop(columns=col_to_remove)"
      ],
      "metadata": {
        "id": "Ej10DzXz9fEI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mer_fin.to_csv('merged.csv', index=False)"
      ],
      "metadata": {
        "id": "sf2Dwm8H9fBP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('merged.csv')\n",
        "data.shape"
      ],
      "metadata": {
        "id": "BccmaMHB9e-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ydata.profiling"
      ],
      "metadata": {
        "id": "_xiIP7WVwQel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "telco_profile = ProfileReport(data, title = 'Telco Profiling Report')"
      ],
      "metadata": {
        "id": "PpFC37c3w6FX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telco_profile"
      ],
      "metadata": {
        "id": "OOiDOFj7w6CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telco_profile.to_file('Report.html')"
      ],
      "metadata": {
        "id": "TBK2NDDpA_9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = telco_profile.to_html()\n",
        "html[:200]"
      ],
      "metadata": {
        "id": "HdEfp2BPA_0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use eval function to convert to a Dictionary\n",
        "json_report = telco_profile.to_json()\n",
        "json_report[:200]"
      ],
      "metadata": {
        "id": "sY5IZzvDA_pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "telco_profile = ProfileReport(data, title = 'Telco Profiling Report', minimal = True)"
      ],
      "metadata": {
        "id": "eds3aE8AA_ao"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(data, train_size = 0.8, stratify=data['Churn_Label'])\n",
        "train_df.shape, test_df.shape"
      ],
      "metadata": {
        "id": "qKlzANd908xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport, compare\n",
        "train_report = ProfileReport(train_df, title='Train Datafrme')\n",
        "test_report = ProfileReport(test_df, title='Test Datafrme')\n",
        "comparison_report = compare([train_report, test_report])"
      ],
      "metadata": {
        "id": "_mclLll008tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_report"
      ],
      "metadata": {
        "id": "A0LSrOGU08qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_report.to_file('Comparison.html')"
      ],
      "metadata": {
        "id": "59Z3MzGP08mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the range of numbers and measures of central tendency for numerical columns\n",
        "\n",
        "# Extract numerical columns\n",
        "numerical_columns = data.select_dtypes(include=['float64']).columns\n",
        "\n",
        "# Display range and measures of central tendency for each numerical column\n",
        "numerical_stats = data[numerical_columns].describe()\n",
        "\n",
        "numerical_stats"
      ],
      "metadata": {
        "id": "YEeHuYglwzyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for special codes or non-nullable fields\n",
        "special_codes = (data == 999).sum()\n",
        "\n",
        "# Check for duplicate records\n",
        "duplicates = data.duplicated().sum()\n",
        "\n",
        "print(special_codes[special_codes > 0]),\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "VhHSjqBow57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Identify outliers using the Tukey method (Boxplot)\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.boxplot(data=data[numerical_columns])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Numerical Columns')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3OQGm_B1w54D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for skewness in the data\n",
        "skewness = data[numerical_columns].skew()\n",
        "\n",
        "skewness"
      ],
      "metadata": {
        "id": "Ev0L5h7D7bTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data['Avg_Monthly_GB_Download_log'] = np.log10(data['Avg_Monthly_GB_Download'] + 1)\n",
        "print(\"Avg_Monthly_GB_Download_log:\\n\", data['Avg_Monthly_GB_Download_log'].skew())\n",
        "\n",
        "data['Total_Extra_Data_Charges'] = np.log10(data['Total_Extra_Data_Charges'] + 1)\n",
        "print(\"\\nTotal_Extra_Data_Charges:\\n\", data['Total_Extra_Data_Charges'].skew())\n",
        "\n",
        "data['Total_Long_Distance_Charges'] = np.log10(data['Total_Long_Distance_Charges'] + 1)\n",
        "print(\"\\nTotal_Long_Distance_Charges:\\n\", data['Total_Long_Distance_Charges'].skew())"
      ],
      "metadata": {
        "id": "SV_ZhHWg7bQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total_Extra_Data_Charges'] = np.log10(data['Total_Extra_Data_Charges'] + 1)\n",
        "print(\"\\nTotal_Extra_Data_Charges:\\n\", data['Total_Extra_Data_Charges'].skew())"
      ],
      "metadata": {
        "id": "nu_SZZ_4gPku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total_Extra_Data_Charges'] = np.log2(data['Total_Extra_Data_Charges'] + 1)\n",
        "print(\"\\nTotal_Extra_Data_Charges:\\n\", data['Total_Extra_Data_Charges'].skew())"
      ],
      "metadata": {
        "id": "F1Hc5gSegh-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize histograms for the numerical columns\n",
        "data[numerical_columns].hist(figsize=(15, 12), bins=50)\n",
        "plt.suptitle('Histograms of Numerical Columns')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Imk5dj-T7bNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize bar charts for the categorical columns\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "fig, axes = plt.subplots(nrows=len(categorical_columns), figsize=(10, 60))\n",
        "\n",
        "for i, col in enumerate(categorical_columns):\n",
        "    sns.countplot(data=data, x=col, ax=axes[i])\n",
        "    axes[i].set_title(f'Bar Chart of {col}')\n",
        "    axes[i].set_ylabel('Count')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2xnIzksR7bJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the relationship between the target variable (Churn_Label) and other variables\n",
        "\n",
        "# Visualize the distribution of the target variable\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data, x='Churn_Label')\n",
        "plt.title('Distribution of Churn_Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A6bDRKRa7bFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the relationship between Churn_Label and categorical variables\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "fig, axes = plt.subplots(nrows=len(categorical_columns) - 1, figsize=(10, 50))  # Exclude Churn_Label itself\n",
        "\n",
        "for i, col in enumerate(categorical_columns):\n",
        "    if col != 'Churn_Label':  # Exclude Churn_Label itself\n",
        "        sns.countplot(data=data, x=col, hue='Churn_Label', ax=axes[i])\n",
        "        axes[i].set_title(f'Relationship between {col} and Churn_Label')\n",
        "        axes[i].set_ylabel('Count')\n",
        "        axes[i].tick_params(axis='x', rotation=45)"
      ],
      "metadata": {
        "id": "jTCdVOJM78dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the relationship between Churn_Label and numerical variables using boxplots\n",
        "fig, axes = plt.subplots(nrows=len(numerical_columns), figsize=(10, 40))\n",
        "\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    sns.boxplot(data=data, x='Churn_Label', y=col, ax=axes[i])\n",
        "    axes[i].set_title(f'Relationship between {col} and Churn_Label')\n",
        "    axes[i].set_ylabel(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X9fkQ--278a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix\n",
        "correlation_matrix = data.corr().round(2)\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1,  fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dzp2duu778Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify pairs of variables with high correlation (greater than 0.7)\n",
        "high_corr_pairs = {}\n",
        "for col in correlation_matrix.columns:\n",
        "    for index in correlation_matrix.index:\n",
        "        if (0.7 < correlation_matrix.loc[index, col] < 1.0) and (col, index) not in high_corr_pairs and (index, col) not in high_corr_pairs:\n",
        "            high_corr_pairs[(index, col)] = correlation_matrix.loc[index, col]\n",
        "\n",
        "high_corr_pairs"
      ],
      "metadata": {
        "id": "uCUbeAHp78VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Analyze the relationship between the target variable and categorical variables using chi-square tests\n",
        "chi2_results = {}\n",
        "for col in categorical_columns:\n",
        "    if col != 'Churn_Label':  # Exclude the target variable itself\n",
        "        contingency_table = pd.crosstab(data[col], data['Churn_Label'])\n",
        "        chi2, p, _, _ = chi2_contingency(contingency_table)\n",
        "        chi2_results[col] = p\n",
        "\n",
        "# Convert the results into a DataFrame for better visualization\n",
        "chi2_df = pd.DataFrame(list(chi2_results.items()), columns=['Variable', 'P-Value'])\n",
        "chi2_df.sort_values(by='P-Value', inplace=True)\n",
        "chi2_df"
      ],
      "metadata": {
        "id": "bLNsbHaa78Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Analyze the relationship between numeric and categorical variables using ANOVA\n",
        "anova_results = {}\n",
        "for cat_col in categorical_columns:\n",
        "    if cat_col != 'Churn_Label':  # Exclude the target variable itself\n",
        "        for num_col in numerical_columns:\n",
        "            groups = [data[num_col][data[cat_col] == category] for category in data[cat_col].unique()]\n",
        "            f_stat, p_value = f_oneway(*groups)\n",
        "            anova_results[(cat_col, num_col)] = p_value\n",
        "\n",
        "# Convert the results into a DataFrame for better visualization\n",
        "anova_df = pd.DataFrame(list(anova_results.items()), columns=['(Categorical, Numerical)', 'P-Value'])\n",
        "anova_df.sort_values(by='P-Value', inplace=True)\n",
        "anova_df.head(10)"
      ],
      "metadata": {
        "id": "dZNmHRtC8flq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "T3sMyATh9e7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Churn_Label'].value_counts()"
      ],
      "metadata": {
        "id": "QTTY59ea_WEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((data['Churn_Label'].value_counts()/len(mer_fin['Churn_Label']))*100)"
      ],
      "metadata": {
        "id": "V4mbbewO_WBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Churn_Label'].value_counts().plot(kind='bar', figsize=(8, 6))\n",
        "plt.xlabel(\"COUNT\", labelpad=16)\n",
        "plt.ylabel(\"CHURN\", labelpad=16)\n",
        "plt.title(\"Count of CHURN\", y=1.00)"
      ],
      "metadata": {
        "id": "SS69Zurf_V-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of key features in relation to Churn_Label\n",
        "\n",
        "features = ['Gender', 'Senior_Citizen', 'Married']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(features, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    sns.countplot(x=feature, hue='Churn_Label', data=data, palette='pastel')\n",
        "    plt.title(f'Distribution of {feature} by Churn_Label')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Churn_Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zEuevqbk_VxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical variables\n",
        "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_features.remove('Churn_Label')  # Remove target variable\n",
        "\n",
        "# Plot bar charts for churn for each categorical variable\n",
        "plt.figure(figsize=(20, 30))\n",
        "for i, feature in enumerate(categorical_features, 1):\n",
        "    plt.subplot(6, 4, i)\n",
        "    sns.countplot(x=feature, hue='Churn_Label', data=data, palette='pastel')\n",
        "    plt.title(f'Distribution of {feature} by Churn_Label')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Churn_Label')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "apLFfrsb_VrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reformat Data - Ordinal\n",
        "# Assign numbers to the 'Contract' column to represent their order\n",
        "contract_mapping = {'Month-to-Month': 1, 'One Year': 2, 'Two Year': 3}\n",
        "data['Contract_Ordinal'] = data['Contract'].map(contract_mapping)\n",
        "data[['Contract', 'Contract_Ordinal']].head(10)"
      ],
      "metadata": {
        "id": "hWzvlDl99YhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove Duplicate Rows\n",
        "initial_rows = data.shape[0]\n",
        "data.drop_duplicates(inplace=True)\n",
        "removed_duplicates = initial_rows - data.shape[0]\n",
        "\n",
        "# 2. Remove Constant Columns\n",
        "constant_columns = [col for col in data.columns if data[col].nunique() == 1]\n",
        "data.drop(columns=constant_columns, inplace=True)\n",
        "\n",
        "removed_duplicates, constant_columns"
      ],
      "metadata": {
        "id": "TJqfRkGg9Ye2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to remove based on high correlation\n",
        "columns_to_remove = ['Referred_a_Friend', 'Total_Charges', 'Internet_Type',\n",
        "                     'Avg_Monthly_GB_Download_log', 'Streaming_Music',\n",
        "                     'Contract_Ordinal', 'Total_Long_Distance_Charges']\n",
        "\n",
        "data.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "# Display the first few rows after removing the columns\n",
        "data.head()\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "s16A2Yo49YNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply label encoding\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Display the first few rows after encoding\n",
        "data.head()"
      ],
      "metadata": {
        "id": "dOX5cxcp9Ybz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "numerical_cols = numerical_cols.drop('Churn_Label')  # Exclude target variable\n",
        "\n",
        "# Apply standard scaling\n",
        "scaler = StandardScaler()\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Display the first few rows after scaling\n",
        "data.head()"
      ],
      "metadata": {
        "id": "hhIr9cFG9YYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data[numerical_cols].corr().round(2)  # Round values to 2 decimal places\n",
        "\n",
        "# Identify pairs of features with high correlation\n",
        "high_corr_pairs = {}\n",
        "for col in correlation_matrix.columns:\n",
        "    for idx in correlation_matrix.index:\n",
        "        if col != idx and abs(correlation_matrix.loc[idx, col]) > 0.7:\n",
        "            sorted_pair = tuple(sorted([col, idx]))\n",
        "            if sorted_pair not in high_corr_pairs:\n",
        "                high_corr_pairs[sorted_pair] = correlation_matrix.loc[idx, col]\n",
        "\n",
        "# Plot the correlation heatmap\n",
        "plt.figure(figsize=(25, 15))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt=\".2f\")  # Use fmt to format annotations\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aihWLgKv9wBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "wpNOnMBAEfPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree with original target distribution and labelled\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Splitting the data into training and testing sets (80% train, 20% test)\n",
        "X = data.drop('Churn_Label', axis=1)\n",
        "y = data['Churn_Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training a simple decision tree model\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "dtclf_ori_accuracy = accuracy_score(y_test, y_pred)\n",
        "dtclf_ori_classification_rep = classification_report(y_test, y_pred)\n",
        "dtclf_ori_confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('The accuracy score for this model is: ', dtclf_ori_accuracy),\n",
        "print('\\nThe classification report for this model: \\n', dtclf_ori_classification_rep),\n",
        "print('\\nThe confusion matrix for this model: \\n', dtclf_ori_confusion_mat)"
      ],
      "metadata": {
        "id": "qTpScih7-JkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree with original target distribution and on dummies\n",
        "\n",
        "# Identifying categorical columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Encoding categorical columns\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Splitting the encoded data into training and testing sets\n",
        "X_train_encoded, X_test_encoded, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the decision tree model on the encoded data\n",
        "clf_encoded = DecisionTreeClassifier(random_state=42)\n",
        "clf_encoded.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_encoded = clf_encoded.predict(X_test_encoded)\n",
        "\n",
        "# Evaluation\n",
        "dtclf_ori_accuracy_encoded = accuracy_score(y_test, y_pred_encoded)\n",
        "dtclf_ori_classification_rep_encoded = classification_report(y_test, y_pred_encoded)\n",
        "dtclf_ori_confusion_mat_encoded = confusion_matrix(y_test, y_pred_encoded)\n",
        "\n",
        "print('The accuracy for this model is: ', dtclf_ori_accuracy_encoded),\n",
        "print('\\nThe classification report for this model: \\n', dtclf_ori_classification_rep_encoded),\n",
        "print('\\nThe confusion matrix for this model: \\n', dtclf_ori_confusion_mat_encoded)"
      ],
      "metadata": {
        "id": "UV6K7P1Z-Jhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "# Initialize the Decision Tree Classifier with a depth of 7\n",
        "dt_classifier_depth7 = DecisionTreeClassifier(max_depth=7, random_state=42)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "dt_classifier_depth7.fit(X_train, y_train)\n",
        "\n",
        "# Plot the decision tree\n",
        "fig, ax = plt.subplots(figsize=(20, 15))\n",
        "tree.plot_tree(dt_classifier_depth7, feature_names=X_train.columns, class_names=['No Churn', 'Churn'], filled=True, rounded=True, fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "74PGTsOX_14q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the model\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Extracting feature importances\n",
        "feature_importances = rf_model.feature_importances_\n",
        "features = X_train_encoded.columns\n",
        "\n",
        "# Creating a DataFrame for feature importances\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting the top 7 features based on importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "importance_df.head(7).plot(kind='bar', x='Feature', y='Importance', legend=False, color='skyblue')\n",
        "plt.title('Top 7 Features based on Importance')\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Feature')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "print('Top 7 Features based on Importance in Descending order are: \\n', importance_df.head(7))"
      ],
      "metadata": {
        "id": "6UE9Y3V5-JM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_model.predict(X_test_encoded)\n",
        "\n",
        "#Random Forest Classifier with encoded data and original target distribution\n",
        "ranfor_ori_accuracy = accuracy_score(y_test, y_pred)\n",
        "ranfor_ori_precision = precision_score(y_test, y_pred)\n",
        "ranfor_ori_recall = recall_score(y_test, y_pred)\n",
        "ranfor_ori_f1 = f1_score(y_test, y_pred)\n",
        "ranfor_ori_roc_auc = roc_auc_score(y_test, y_pred)\n",
        "ranfor_ori_classification_rep = classification_report(y_test, y_pred)\n",
        "ranfor_ori_confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('The accuracy for this model is: ', ranfor_ori_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', ranfor_ori_precision),\n",
        "print('\\nThe recall score for this model: \\n', ranfor_ori_recall),\n",
        "print('\\nThe F1 score for this model: \\n', ranfor_ori_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', ranfor_ori_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', ranfor_ori_classification_rep),\n",
        "print('\\nThe confusion matrix for this model: \\n', ranfor_ori_confusion_mat)"
      ],
      "metadata": {
        "id": "U21ea3LX2PP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Splitting the data into training and testing sets (80% train, 20% test)\n",
        "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(X_train_encoded, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training a Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train_simple, y_train_simple)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_simple = log_reg.predict(X_test_simple)\n",
        "\n",
        "# Logistic Regression with encoded data and original target distribution\n",
        "logreg_ori_accuracy = accuracy_score(y_test_simple, y_pred_simple)\n",
        "logreg_ori_precision = precision_score(y_test_simple, y_pred_simple)\n",
        "logreg_ori_recall = recall_score(y_test_simple, y_pred_simple)\n",
        "logreg_ori_f1 = f1_score(y_test_simple, y_pred_simple)\n",
        "logreg_ori_roc_auc = roc_auc_score(y_test_simple, y_pred_simple)\n",
        "logreg_ori_classification_rep = classification_report(y_test_simple, y_pred_simple)\n",
        "logreg_ori_conf_matrix = confusion_matrix(y_test_simple, y_pred_simple)\n",
        "\n",
        "print('The accuracy for this model is: ', logreg_ori_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', logreg_ori_precision),\n",
        "print('\\nThe recall score for this model: \\n', logreg_ori_recall),\n",
        "print('\\nThe F1 score for this model: \\n', logreg_ori_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', logreg_ori_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', logreg_ori_classification_rep),\n",
        "print('\\nThe confusion matrix for this model: \\n', logreg_ori_conf_matrix)"
      ],
      "metadata": {
        "id": "pMElQbQZ_K-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the correlation of each feature with the target variable\n",
        "correlation_with_target = X_train_encoded.join(y_train).corr()['Churn_Label'].sort_values(ascending=False)\n",
        "correlation_with_target"
      ],
      "metadata": {
        "id": "hWMLfnIV_K7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Decision Tree Classifier with a depth of 5\n",
        "dt_classifier_depth5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "dt_classifier_depth5.fit(X_train, y_train)\n",
        "\n",
        "# Plot the decision tree\n",
        "fig, ax = plt.subplots(figsize=(20, 15))\n",
        "tree.plot_tree(dt_classifier_depth5, feature_names=X_train.columns, class_names=['No Churn', 'Churn'], filled=True, rounded=True, fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LJ7cVqMv_12P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict on the test dataset\n",
        "y_pred_depth5 = dt_classifier_depth5.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "dtclf_ori_accuracy_depth5 = accuracy_score(y_test, y_pred_depth5)\n",
        "dtclf_ori_precision_depth5 = precision_score(y_test, y_pred_depth5)\n",
        "dtclf_ori_recall_depth5 = recall_score(y_test, y_pred_depth5)\n",
        "dtclf_ori_f1_depth5 = f1_score(y_test, y_pred_depth5)\n",
        "dtclf_ori_roc_auc_depth5 = roc_auc_score(y_test, y_pred_depth5)\n",
        "dtclf_ori_classification_rep_depth5 = classification_report(y_test, y_pred_depth5)\n",
        "dtclf_ori_conf_matrix_depth5 = confusion_matrix(y_test, y_pred_depth5)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(dtclf_ori_conf_matrix_depth5, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title(f'Confusion Matrix (Accuracy: {dtclf_ori_accuracy_depth5:.2f})')\n",
        "plt.show()\n",
        "\n",
        "print('The accuracy for this model is: ', dtclf_ori_accuracy_depth5),\n",
        "print('\\nThe precision score for this model: \\n', dtclf_ori_precision_depth5),\n",
        "print('\\nThe recall score for this model: \\n', dtclf_ori_recall_depth5),\n",
        "print('\\nThe F1 score for this model: \\n', dtclf_ori_f1_depth5),\n",
        "print('\\nThe roc_auc score for this model: \\n', dtclf_ori_roc_auc_depth5),\n",
        "print('\\nThe classification report for this model: \\n', dtclf_ori_classification_rep_depth5),\n",
        "print('\\nThe confusion matrix for this model: \\n', dtclf_ori_conf_matrix_depth5)"
      ],
      "metadata": {
        "id": "NXzT6RIo_1y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "hbb2k7Q-_1vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict churn labels for the test set\n",
        "y_pred = logreg_model.predict(X_test)\n",
        "\n",
        "# Logistic Regression with encoded data and original target distribution\n",
        "logreg_ori_accuracy = accuracy_score(y_test, y_pred)\n",
        "logreg_ori_precision = precision_score(y_test, y_pred)\n",
        "logreg_ori_recall = recall_score(y_test, y_pred)\n",
        "logreg_ori_f1 = f1_score(y_test, y_pred)\n",
        "logreg_ori_roc_auc = roc_auc_score(y_test, y_pred)\n",
        "logreg_ori_classification_rep = classification_report(y_test, y_pred)\n",
        "logreg_ori_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(logreg_ori_conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Not Churned', 'Churned'],\n",
        "            yticklabels=['Not Churned', 'Churned'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print('The accuracy for this model is: ', logreg_ori_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', logreg_ori_precision),\n",
        "print('\\nThe recall score for this model: \\n', logreg_ori_recall),\n",
        "print('\\nThe F1 score for this model: \\n', logreg_ori_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', logreg_ori_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', logreg_ori_classification_rep),\n",
        "print('\\nThe confusion matrix for this model: \\n', logreg_ori_conf_matrix)"
      ],
      "metadata": {
        "id": "aTeng2aK_1rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming y_test and y_pred are already defined\n",
        "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "print(report_df)\n"
      ],
      "metadata": {
        "id": "2aXW4_ID_1nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Categorical Variables using One-Hot Encoding\n",
        "data_encoded = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "# Display the first few rows of the encoded dataset\n",
        "data_encoded.head()"
      ],
      "metadata": {
        "id": "vm9QdD9G_VoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q imbalanced-learn"
      ],
      "metadata": {
        "id": "y54CK1AzNKsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Splitting features and target variable\n",
        "X = data_encoded.drop('Churn_Label', axis=1)\n",
        "y = data_encoded['Churn_Label']\n",
        "\n",
        "# Applying SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Checking the distribution of the target variable after SMOTE\n",
        "y_resampled.value_counts()"
      ],
      "metadata": {
        "id": "qKLjBb0f_Vk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the resampled features\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "# Display the first few rows of the scaled features\n",
        "X_resampled_scaled[:5]"
      ],
      "metadata": {
        "id": "uU5zOxW-_Vhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shape of the training and testing sets\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "Ds42wmm-_VeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "\n",
        "# Logistic Regression after SMOTE\n",
        "logreg_smo_accuracy = accuracy_score(y_test, y_pred_logreg)\n",
        "logreg_smo_precision = precision_score(y_test, y_pred_logreg)\n",
        "logreg_smo_recall = recall_score(y_test, y_pred_logreg)\n",
        "logreg_smo_f1 = f1_score(y_test, y_pred_logreg)\n",
        "logreg_smo_roc_auc = roc_auc_score(y_test, y_pred_logreg)\n",
        "logreg_smo_classification_report = classification_report(y_test, y_pred_logreg)\n",
        "logreg_smo_conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
        "\n",
        "print('The accuracy for this model is: ', logreg_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', logreg_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', logreg_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', logreg_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', logreg_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', logreg_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', logreg_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "YP49rmQB_Vaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and train the Decision Trees model\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_dtree = dtree.predict(X_test)\n",
        "\n",
        "# Decision tree after SMOTE\n",
        "dtclf_smo_accuracy = accuracy_score(y_test, y_pred_dtree)\n",
        "dtclf_smo_precision = precision_score(y_test, y_pred_dtree)\n",
        "dtclf_smo_recall = recall_score(y_test, y_pred_dtree)\n",
        "dtclf_smo_f1 = f1_score(y_test, y_pred_dtree)\n",
        "dtclf_smo_roc_auc = roc_auc_score(y_test, y_pred_dtree)\n",
        "dtclf_smo_classification_report = classification_report(y_test, y_pred_dtree)\n",
        "dtclf_smo_conf_matrix = confusion_matrix(y_test, y_pred_dtree)\n",
        "\n",
        "print('The accuracy for this model is: ', dtclf_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', dtclf_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', dtclf_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', dtclf_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', dtclf_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', dtclf_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', dtclf_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "dlL0RtHTkX8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xgboost"
      ],
      "metadata": {
        "id": "2W4Vy15KkX2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Random Forest Classifier after SMOTE\n",
        "ranfor_smo_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "ranfor_smo_precision = precision_score(y_test, y_pred_rf)\n",
        "ranfor_smo_recall = recall_score(y_test, y_pred_rf)\n",
        "ranfor_smo_f1 = f1_score(y_test, y_pred_rf)\n",
        "ranfor_smo_roc_auc = roc_auc_score(y_test, y_pred_rf)\n",
        "ranfor_smo_classification_report = classification_report(y_test, y_pred_rf)\n",
        "ranfor_smo_conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "\n",
        "print('The accuracy for this model is: ', ranfor_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', ranfor_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', ranfor_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', ranfor_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', ranfor_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', ranfor_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', ranfor_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "GZ7HWln9kX5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, n_estimators=100)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# XGBClassifier after SMOTE\n",
        "xgb_smo_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_smo_precision = precision_score(y_test, y_pred_xgb)\n",
        "xgb_smo_recall = recall_score(y_test, y_pred_xgb)\n",
        "xgb_smo_f1 = f1_score(y_test, y_pred_xgb)\n",
        "xgb_smo_roc_auc = roc_auc_score(y_test, y_pred_xgb)\n",
        "xgb_smo_classification_report = classification_report(y_test, y_pred_xgb)\n",
        "xgb_smo_conf_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "print('The accuracy for this model is: ', xgb_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', xgb_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', xgb_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', xgb_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', xgb_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', xgb_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', xgb_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "7qWpf8d-kX0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize and train the SVM model\n",
        "svm_model = SVC(random_state=42, probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# SVC after SMOTE\n",
        "svm_smo_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_smo_precision = precision_score(y_test, y_pred_svm)\n",
        "svm_smo_recall = recall_score(y_test, y_pred_svm)\n",
        "svm_smo_f1 = f1_score(y_test, y_pred_svm)\n",
        "svm_smo_roc_auc = roc_auc_score(y_test, y_pred_svm)\n",
        "svm_smo_classification_report = classification_report(y_test, y_pred_svm)\n",
        "svm_smo_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
        "\n",
        "\n",
        "print('The accuracy for this model is: ', svm_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', svm_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', svm_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', svm_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', svm_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', svm_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', svm_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "RIQ7_fgWkXxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "lzfVWOKp5LnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "\n",
        "# Define the Neural Network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy',\n",
        "                       Precision(name='precision'),\n",
        "                       Recall(name='recall'),\n",
        "                       AUC(name='auc_roc', curve='ROC'),  # AUC for ROC curve\n",
        "                       AUC(name='auc_pr', curve='PR')    # AUC for Precision-Recall curve\n",
        "                       ]\n",
        "            )\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "gG2frNjhkXuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the history.history dictionary to a pandas DataFrame\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Optionally, add an epoch column\n",
        "history_df['epoch'] = history.epoch\n",
        "\n",
        "# Display the DataFrame\n",
        "print(history_df)"
      ],
      "metadata": {
        "id": "OEsiL9MUvqYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "history_df.to_csv('training_history.csv', index=False)\n",
        "\n",
        "# Save to Excel\n",
        "history_df.to_excel('training_history.xlsx', index=False)"
      ],
      "metadata": {
        "id": "pcgXlxBLwjbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setting up the size of the plots\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Plotting Loss\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "# Plotting Precision\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "plt.title('Model Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Plotting Recall\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot(history.history['recall'])\n",
        "plt.plot(history.history['val_recall'])\n",
        "plt.title('Model Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Plotting AUC for ROC\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.plot(history.history['auc_roc'])\n",
        "plt.plot(history.history['val_auc_roc'])\n",
        "plt.title('Model AUC for ROC')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Plotting AUC for PR\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.plot(history.history['auc_pr'])\n",
        "plt.plot(history.history['val_auc_pr'])\n",
        "plt.title('Model AUC for PR')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LXWCowyoyDTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'subsample': [0.5, 0.7, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print('The best parameters are : \\n', best_params)"
      ],
      "metadata": {
        "id": "sHQgClo6kXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # Using 5 neighbors for KNN\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "knn_preds = knn.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "# KNN after SMOTE\n",
        "knn_smo_accuracy = accuracy_score(y_test, knn_preds)\n",
        "knn_smo_precision = precision_score(y_test, knn_preds)\n",
        "knn_smo_recall = recall_score(y_test, knn_preds)\n",
        "knn_smo_f1 = f1_score(y_test, knn_preds)\n",
        "knn_smo_roc_auc = roc_auc_score(y_test, knn_preds)\n",
        "knn_smo_classification_report = classification_report(y_test, knn_preds)\n",
        "knn_smo_conf_matrix = confusion_matrix(y_test, knn_preds)\n",
        "\n",
        "print('The knn_accuracy is         : ', knn_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', knn_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', knn_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', knn_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', knn_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', knn_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', knn_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "uGEnazoBLj8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "naive_bayes = GaussianNB()\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "naive_bayes_preds = naive_bayes.predict(X_test)\n",
        "\n",
        "# Naive Bayes after SMOTE\n",
        "naive_bayes_smo_accuracy = accuracy_score(y_test, naive_bayes_preds)\n",
        "naive_bayes_smo_precision = precision_score(y_test, naive_bayes_preds)\n",
        "naive_bayes_smo_recall = recall_score(y_test, naive_bayes_preds)\n",
        "naive_bayes_smo_f1 = f1_score(y_test, naive_bayes_preds)\n",
        "naive_bayes_smo_roc_auc = roc_auc_score(y_test, naive_bayes_preds)\n",
        "naive_bayes_smo_classification_report = classification_report(y_test, naive_bayes_preds)\n",
        "naive_bayes_smo_conf_matrix = confusion_matrix(y_test, naive_bayes_preds)\n",
        "\n",
        "print('The naive_bayes_accuracy is : ', naive_bayes_smo_accuracy),\n",
        "print('\\nThe precision score for this model: \\n', naive_bayes_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', naive_bayes_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', naive_bayes_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', naive_bayes_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', naive_bayes_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', naive_bayes_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "QHmcraLa30yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "ada_boost = AdaBoostClassifier(n_estimators=50)  # Using 50 estimators for AdaBoost\n",
        "ada_boost.fit(X_train, y_train)\n",
        "\n",
        "ada_boost_preds = ada_boost.predict(X_test)\n",
        "\n",
        "# ADA Boost after SMOTE\n",
        "ada_boost_smo_accuracy = accuracy_score(y_test, ada_boost_preds)\n",
        "ada_boost_smo_precision = precision_score(y_test, ada_boost_preds)\n",
        "ada_boost_smo_recall = recall_score(y_test, ada_boost_preds)\n",
        "ada_boost_smo_f1 = f1_score(y_test, ada_boost_preds)\n",
        "ada_boost_smo_roc_auc = roc_auc_score(y_test, ada_boost_preds)\n",
        "ada_boost_smo_classification_report = classification_report(y_test, ada_boost_preds)\n",
        "ada_boost_smo_conf_matrix = confusion_matrix(y_test, ada_boost_preds)\n",
        "\n",
        "\n",
        "print('The ada_boost_accuracy is   : ', ada_boost_smo_accuracy)\n",
        "print('\\nThe precision score for this model: \\n', ada_boost_smo_precision),\n",
        "print('\\nThe recall score for this model: \\n', ada_boost_smo_recall),\n",
        "print('\\nThe F1 score for this model: \\n', ada_boost_smo_f1),\n",
        "print('\\nThe roc_auc score for this model: \\n', ada_boost_smo_roc_auc),\n",
        "print('\\nThe classification report for this model: \\n', ada_boost_smo_classification_report),\n",
        "print('\\nThe confusion matrix for this model: \\n', ada_boost_smo_conf_matrix)"
      ],
      "metadata": {
        "id": "jNJGo3mE30w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "ZfvaX-7fojTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Sample data (only a snippet for demonstration)\n",
        "data = pd.read_csv('merged.csv')\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Split data\n",
        "X = df.drop('Churn_Label', axis=1)\n",
        "y = df['Churn_Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForest model\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2BGd9eWGmaCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the \"STATISTICAL IMPORTANCE\" of the Variables.\n",
        "# Define a label encoder\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "#from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "# Loop through each column in the dataframe and if the column type is object (categorical feature),\n",
        "# we will use the label encoder to transform it into numeric values\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        data[col] = labelencoder.fit_transform(data[col])\n",
        "\n",
        "# Define features X (all columns except 'Churn_Label') and target y ('Churn_Label')\n",
        "X = data.drop('Churn_Label', axis=1)\n",
        "y = data['Churn_Label']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "features = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by descending importance\n",
        "features = features.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(features)"
      ],
      "metadata": {
        "id": "pli-eMrhkXoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LimeTabularExplainer instance\n",
        "explainer = LimeTabularExplainer(X_train.values,\n",
        "                                 mode='classification',\n",
        "                                 training_labels=y_train,\n",
        "                                 feature_names=X_train.columns.tolist(),\n",
        "                                 class_names=label_encoders['Churn_Label'].classes_.tolist(),\n",
        "                                 discretize_continuous=True)"
      ],
      "metadata": {
        "id": "PhSJJbcBmZ64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "id": "XOsL3ASyhkHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of variables for an individual prediction\n",
        "\n",
        "# Prompt the user for an observation number\n",
        "example = int(input('The Test dataset has 2113 observations. Enter the observation number please and press enter: '))\n",
        "\n",
        "# Ensure the example number is within the range of the test dataset\n",
        "if 0 <= example < len(X_test):\n",
        "    exp = explainer.explain_instance(X_test.iloc[example].values, clf.predict_proba)\n",
        "    exp.show_in_notebook(show_table=True, show_all=False)\n",
        "else:\n",
        "    print(\"Invalid observation number. Please enter a number between 0 and \", len(X_test)-1)"
      ],
      "metadata": {
        "id": "Ts6x4jBurn8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.as_map()"
      ],
      "metadata": {
        "id": "sxwrJ85CNDgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a list of dictionaries with model names and their metrics\n",
        "models_evaluation = [\n",
        "\n",
        "    {'Model': 'DeTrClassifier Pre SMOTE', 'Accuracy': dtclf_ori_accuracy,  'Precision': 'N / A',\n",
        "     'Recall': 'N / A', 'F1': 'N / A', 'ROC_AUC' : 'N / A', 'Confusion Matrix': dtclf_ori_confusion_mat},\n",
        "\n",
        "    {'Model': 'DeTrCalssifier Pre SMOTE Encoded', 'Accuracy': dtclf_ori_accuracy_encoded, 'Precision': 'N / A',\n",
        "     'Recall': 'N / A', 'F1': 'N / A', 'ROC_AUC' : 'N / A', 'Confusion Matrix': dtclf_ori_confusion_mat_encoded},\n",
        "\n",
        "    {'Model': 'RaFoClassifier Pre SMOTE Encoded', 'Accuracy': ranfor_ori_accuracy, 'Precision': ranfor_ori_precision,\n",
        "     'Recall': ranfor_ori_recall, 'F1': ranfor_ori_f1, 'ROC_AUC' : ranfor_ori_roc_auc, 'Confusion Matrix': ranfor_ori_confusion_mat},\n",
        "\n",
        "    {'Model': 'LogRegression Pre SMOTE Encoded', 'Accuracy': logreg_ori_accuracy, 'Precision': logreg_ori_precision,\n",
        "     'Recall': logreg_ori_recall, 'F1': logreg_ori_f1, 'ROC_AUC' : logreg_ori_roc_auc, 'Confusion Matrix': logreg_ori_conf_matrix},\n",
        "\n",
        "    {'Model': 'DeTrClassifier depth5 Pre SMOTE', 'Accuracy': dtclf_ori_accuracy_depth5, 'Precision': dtclf_ori_precision_depth5,\n",
        "     'Recall': dtclf_ori_recall_depth5, 'F1': dtclf_ori_f1_depth5, 'ROC_AUC' : dtclf_ori_roc_auc_depth5,\n",
        "     'Confusion Matrix': dtclf_ori_conf_matrix_depth5},\n",
        "\n",
        "    {'Model': 'LogRegression Pre SMOTE)', 'Accuracy': logreg_ori_accuracy, 'Precision': logreg_ori_precision,\n",
        "     'Recall': logreg_ori_recall, 'F1': logreg_ori_f1, 'ROC_AUC' : logreg_ori_roc_auc, 'Confusion Matrix': logreg_ori_conf_matrix},\n",
        "\n",
        "    {'Model': 'LogRegression after SMOTE', 'Accuracy': logreg_smo_accuracy, 'Precision': logreg_smo_precision,\n",
        "     'Recall': logreg_smo_recall, 'F1': logreg_smo_f1, 'ROC_AUC' : logreg_smo_roc_auc, 'Confusion Matrix': logreg_smo_conf_matrix},\n",
        "\n",
        "    {'Model': 'Decision tree after SMOTE', 'Accuracy': dtclf_smo_accuracy, 'Precision': dtclf_smo_precision,\n",
        "     'Recall': dtclf_smo_recall, 'F1': dtclf_smo_f1, 'ROC_AUC' : dtclf_smo_roc_auc, 'Confusion Matrix': dtclf_smo_conf_matrix},\n",
        "\n",
        "    {'Model': 'RanForClassifier after SMOTE', 'Accuracy': ranfor_smo_accuracy, 'Precision': ranfor_smo_precision,\n",
        "     'Recall': ranfor_smo_recall, 'F1': ranfor_smo_f1, 'ROC_AUC' : ranfor_smo_roc_auc, 'Confusion Matrix': ranfor_smo_conf_matrix},\n",
        "\n",
        "    {'Model': 'XGBClassifier after SMOTE', 'Accuracy': xgb_smo_accuracy, 'Precision': xgb_smo_precision,\n",
        "     'Recall': xgb_smo_recall, 'F1': xgb_smo_f1, 'ROC_AUC' : xgb_smo_roc_auc, 'Confusion Matrix': xgb_smo_conf_matrix},\n",
        "\n",
        "     {'Model': 'SVC after SMOTE', 'Accuracy': svm_smo_accuracy, 'Precision': svm_smo_precision,\n",
        "     'Recall': svm_smo_recall, 'F1': svm_smo_f1, 'ROC_AUC' : svm_smo_roc_auc, 'Confusion Matrix': svm_smo_conf_matrix},\n",
        "\n",
        "     {'Model': 'KNN after SMOTE', 'Accuracy': knn_smo_accuracy, 'Precision': knn_smo_precision,\n",
        "     'Recall': knn_smo_recall, 'F1': knn_smo_f1, 'ROC_AUC' : knn_smo_roc_auc, 'Confusion Matrix': knn_smo_conf_matrix},\n",
        "\n",
        "     {'Model': 'Naive Bayes after SMOTE', 'Accuracy': naive_bayes_smo_accuracy, 'Precision': naive_bayes_smo_precision,\n",
        "     'Recall': naive_bayes_smo_recall, 'F1': naive_bayes_smo_f1, 'ROC_AUC' : naive_bayes_smo_roc_auc,\n",
        "     'Confusion Matrix': naive_bayes_smo_conf_matrix},\n",
        "\n",
        "     {'Model': 'ADA Boost after SMOTE', 'Accuracy': ada_boost_smo_accuracy, 'Precision': ada_boost_smo_precision,\n",
        "     'Recall': ada_boost_smo_recall, 'F1': ada_boost_smo_f1, 'ROC_AUC' : ada_boost_smo_roc_auc, 'Confusion Matrix': ada_boost_smo_conf_matrix},\n",
        "\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(models_evaluation)\n",
        "\n",
        "# Set the display width to a large value\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "7DQgEcUvQf9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('model_scores.csv', index=False)"
      ],
      "metadata": {
        "id": "rfEC6rI2_ocG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('merged.csv')\n",
        "\n",
        "acceptable_ranges = {}\n",
        "\n",
        "for column in data.columns:\n",
        "    if data[column].dtype in ['int64', 'float64']:\n",
        "        Q1 = data[column].quantile(0.25)\n",
        "        Q3 = data[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        acceptable_ranges[column] = (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
        "    else:\n",
        "        acceptable_ranges[column] = data[column].unique()\n",
        "acceptable_ranges"
      ],
      "metadata": {
        "id": "ESCR-FHPeye3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caps_and_floors = {}\n",
        "\n",
        "for column in data.columns:\n",
        "    if data[column].dtype in ['int64', 'float64']:\n",
        "        cap = data[column].quantile(0.99)\n",
        "        floor = data[column].quantile(0.01)\n",
        "        caps_and_floors[column] = (floor, cap)\n",
        "caps_and_floors"
      ],
      "metadata": {
        "id": "vqHaSFIoeyb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp, chisquare\n",
        "\n",
        "def check_drift(original_data, new_data):\n",
        "    drift_columns = []\n",
        "    for column in original_data.columns:\n",
        "        if original_data[column].dtype in ['int64', 'float64']:\n",
        "            statistic, p_value = ks_2samp(original_data[column], new_data[column])\n",
        "            if p_value < 0.05:\n",
        "                drift_columns.append(column)\n",
        "        else:\n",
        "            observed = new_data[column].value_counts().reindex(original_data[column].unique()).fillna(0)\n",
        "            expected = original_data[column].value_counts().reindex(original_data[column].unique()).fillna(0)\n",
        "            statistic, p_value = chisquare(observed, expected)\n",
        "            if p_value < 0.05:\n",
        "                drift_columns.append(column)\n",
        "    return drift_columns"
      ],
      "metadata": {
        "id": "5Iq0poSKeyYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}